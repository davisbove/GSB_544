{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VsR2S6DwjXM1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import make_column_selector, ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data\n",
        "ames = pd.read_csv(\"AmesHousing.csv\")\n",
        "\n",
        "# Get rid of columns with mostly NaN values\n",
        "good_cols = ames.isna().sum() < 100\n",
        "ames = ames.loc[:,good_cols]\n",
        "\n",
        "# Drop other NAs\n",
        "ames = ames.dropna()"
      ],
      "metadata": {
        "id": "h8Fqoi1dkyo3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = ames.drop([\"SalePrice\", \"Order\", \"PID\"], axis = 1)\n",
        "y = ames[\"SalePrice\"]\n",
        "\n",
        "\n",
        "ct = ColumnTransformer(\n",
        "  [\n",
        "    (\"dummify\",\n",
        "    OneHotEncoder(sparse_output = False, handle_unknown='ignore'),\n",
        "    make_column_selector(dtype_include=object)),\n",
        "    (\"standardize\",\n",
        "    StandardScaler(),\n",
        "    make_column_selector(dtype_include=np.number))\n",
        "  ],\n",
        "  remainder = \"passthrough\"\n",
        ")\n",
        "\n",
        "lr_pipeline_1 = Pipeline(\n",
        "  [(\"preprocessing\", ct),\n",
        "  (\"linear_regression\", LinearRegression())]\n",
        ")"
      ],
      "metadata": {
        "id": "cqB0tYuMk1PA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_val_score(lr_pipeline_1, X, y, cv = 5, scoring = 'r2')\n",
        "#large negative numbers indicate a crazy overfitting of the data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ftcb5qa4k3kL",
        "outputId": "14ced896-37ae-4907-9401-74a4a1c3eaf2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.59303720e+21, -1.13145211e+19, -7.57138616e+20, -4.47669752e+18,\n",
              "       -2.55949915e+20])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_pipeline_2 = Pipeline(\n",
        "  [(\"preprocessing\", ct),\n",
        "  (\"linear_regression\", Ridge())]\n",
        ")\n",
        "\n",
        "cross_val_score(lr_pipeline_2, X, y, cv = 5, scoring = 'r2')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_XtkLD7lBSk",
        "outputId": "01f0bc9c-ef65-479c-f9df-1de0175ea1d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.89815807, 0.91744024, 0.79493606, 0.78522563, 0.91389818])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "lr_pipeline_2 = Pipeline(\n",
        "  [(\"preprocessing\", ct),\n",
        "  (\"linear_regression\", Ridge())]\n",
        ")\n",
        "\n",
        "degrees = {'linear_regression__alpha': np.logspace(-3, 1, num=5) }\n",
        "\n",
        "gscv = GridSearchCV(lr_pipeline_2, degrees, cv = 5, scoring='r2')\n"
      ],
      "metadata": {
        "id": "CA8nvRMhlpxp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbDjhwDGo8IV",
        "outputId": "2ac71b27-269a-42f5-a347-0d71a7db9005"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'linear_regression': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01])}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gscv_fitted = gscv.fit(X, y)\n",
        "\n",
        "gscv_fitted.cv_results_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hHAcyWSnvs7",
        "outputId": "26edd58f-fc5e-49a7-e86f-de83fce20062"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.517e+11, tolerance: 1.348e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.311e+11, tolerance: 1.474e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.147e+11, tolerance: 1.463e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.396e+11, tolerance: 1.407e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.286e+11, tolerance: 1.477e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.380e+11, tolerance: 1.348e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.170e+11, tolerance: 1.474e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.082e+11, tolerance: 1.463e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.278e+11, tolerance: 1.407e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.151e+11, tolerance: 1.477e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.191e+11, tolerance: 1.348e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.975e+11, tolerance: 1.474e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.005e+11, tolerance: 1.463e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.109e+11, tolerance: 1.407e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.963e+11, tolerance: 1.477e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.886e+11, tolerance: 1.348e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.657e+11, tolerance: 1.474e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.906e+11, tolerance: 1.463e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.824e+11, tolerance: 1.407e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.656e+11, tolerance: 1.477e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.109e+11, tolerance: 1.348e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+11, tolerance: 1.474e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.894e+11, tolerance: 1.463e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.756e+11, tolerance: 1.407e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.569e+11, tolerance: 1.477e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.782e+11, tolerance: 1.348e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.645e+11, tolerance: 1.474e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.090e+11, tolerance: 1.463e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.465e+11, tolerance: 1.407e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.590e+11, tolerance: 1.477e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.429e+09, tolerance: 1.474e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.015e+10, tolerance: 1.463e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.603e+09, tolerance: 1.407e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.311e+09, tolerance: 1.477e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.608e+11, tolerance: 1.348e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.375e+11, tolerance: 1.474e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.099e+11, tolerance: 1.463e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.468e+11, tolerance: 1.407e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.330e+11, tolerance: 1.477e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.110e+11, tolerance: 1.348e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e+11, tolerance: 1.474e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.988e+11, tolerance: 1.463e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.757e+11, tolerance: 1.407e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.456e+11, tolerance: 1.477e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.053e+12, tolerance: 1.348e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.163e+12, tolerance: 1.474e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.783e+11, tolerance: 1.463e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.013e+11, tolerance: 1.407e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.151e+12, tolerance: 1.477e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.236e+11, tolerance: 1.348e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.570e+11, tolerance: 1.474e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.588e+11, tolerance: 1.463e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.698e+11, tolerance: 1.407e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.557e+11, tolerance: 1.477e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.919e+12, tolerance: 1.348e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.089e+12, tolerance: 1.474e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.921e+12, tolerance: 1.463e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.790e+12, tolerance: 1.407e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.074e+12, tolerance: 1.477e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.323e+10, tolerance: 1.477e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.468e+12, tolerance: 1.348e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.860e+12, tolerance: 1.474e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.754e+12, tolerance: 1.463e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.545e+12, tolerance: 1.407e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.857e+12, tolerance: 1.477e+09 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.064e+09, tolerance: 1.793e+09\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([1.33802881, 0.81871724, 0.81581178, 1.44770331, 0.84443917,\n",
              "        1.21154027, 0.88926725, 0.76802297, 1.06707673, 1.0889504 ,\n",
              "        0.87793756, 0.18248401, 0.46044827, 0.61236515, 0.92429557,\n",
              "        1.62563429, 0.12389855, 0.14770436, 0.14092731, 0.70239983,\n",
              "        1.46591191, 0.1063818 , 0.11967888, 0.11260509, 0.28916807]),\n",
              " 'std_fit_time': array([0.73250309, 0.07476696, 0.04186669, 0.78395608, 0.01590609,\n",
              "        0.8176724 , 0.37072495, 0.03042483, 0.48252685, 0.47912355,\n",
              "        0.01876917, 0.0108315 , 0.14809504, 0.40851639, 0.2975515 ,\n",
              "        0.62588885, 0.01165604, 0.00412696, 0.01424078, 0.11146847,\n",
              "        0.693045  , 0.01196052, 0.02456815, 0.01911365, 0.08681502]),\n",
              " 'mean_score_time': array([0.04262409, 0.03062572, 0.03024802, 0.03340044, 0.02737274,\n",
              "        0.03202233, 0.02851195, 0.03063149, 0.04079208, 0.03170118,\n",
              "        0.0284421 , 0.0285871 , 0.04939809, 0.03707933, 0.03201141,\n",
              "        0.03262916, 0.03458786, 0.02898035, 0.02872148, 0.02991028,\n",
              "        0.03064165, 0.02725606, 0.0321146 , 0.03249264, 0.0319479 ]),\n",
              " 'std_score_time': array([0.00938005, 0.00198528, 0.00352504, 0.00869839, 0.00076657,\n",
              "        0.0048739 , 0.00190809, 0.00215476, 0.01391602, 0.00397883,\n",
              "        0.00174639, 0.00139064, 0.01619509, 0.0142675 , 0.00250251,\n",
              "        0.00810707, 0.0114396 , 0.00178887, 0.00225225, 0.00131369,\n",
              "        0.00449898, 0.00133447, 0.00478051, 0.00744393, 0.00714861]),\n",
              " 'param_linear_regression__alpha': masked_array(data=[0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01,\n",
              "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 1.0, 1.0, 1.0,\n",
              "                    1.0, 1.0, 10.0, 10.0, 10.0, 10.0, 10.0],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False],\n",
              "        fill_value=1e+20),\n",
              " 'param_linear_regression__l1_ratio': masked_array(data=[0.0, 0.25, 0.5, 0.75, 1.0, 0.0, 0.25, 0.5, 0.75, 1.0,\n",
              "                    0.0, 0.25, 0.5, 0.75, 1.0, 0.0, 0.25, 0.5, 0.75, 1.0,\n",
              "                    0.0, 0.25, 0.5, 0.75, 1.0],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False],\n",
              "        fill_value=1e+20),\n",
              " 'params': [{'linear_regression__alpha': 0.001,\n",
              "   'linear_regression__l1_ratio': 0.0},\n",
              "  {'linear_regression__alpha': 0.001, 'linear_regression__l1_ratio': 0.25},\n",
              "  {'linear_regression__alpha': 0.001, 'linear_regression__l1_ratio': 0.5},\n",
              "  {'linear_regression__alpha': 0.001, 'linear_regression__l1_ratio': 0.75},\n",
              "  {'linear_regression__alpha': 0.001, 'linear_regression__l1_ratio': 1.0},\n",
              "  {'linear_regression__alpha': 0.01, 'linear_regression__l1_ratio': 0.0},\n",
              "  {'linear_regression__alpha': 0.01, 'linear_regression__l1_ratio': 0.25},\n",
              "  {'linear_regression__alpha': 0.01, 'linear_regression__l1_ratio': 0.5},\n",
              "  {'linear_regression__alpha': 0.01, 'linear_regression__l1_ratio': 0.75},\n",
              "  {'linear_regression__alpha': 0.01, 'linear_regression__l1_ratio': 1.0},\n",
              "  {'linear_regression__alpha': 0.1, 'linear_regression__l1_ratio': 0.0},\n",
              "  {'linear_regression__alpha': 0.1, 'linear_regression__l1_ratio': 0.25},\n",
              "  {'linear_regression__alpha': 0.1, 'linear_regression__l1_ratio': 0.5},\n",
              "  {'linear_regression__alpha': 0.1, 'linear_regression__l1_ratio': 0.75},\n",
              "  {'linear_regression__alpha': 0.1, 'linear_regression__l1_ratio': 1.0},\n",
              "  {'linear_regression__alpha': 1.0, 'linear_regression__l1_ratio': 0.0},\n",
              "  {'linear_regression__alpha': 1.0, 'linear_regression__l1_ratio': 0.25},\n",
              "  {'linear_regression__alpha': 1.0, 'linear_regression__l1_ratio': 0.5},\n",
              "  {'linear_regression__alpha': 1.0, 'linear_regression__l1_ratio': 0.75},\n",
              "  {'linear_regression__alpha': 1.0, 'linear_regression__l1_ratio': 1.0},\n",
              "  {'linear_regression__alpha': 10.0, 'linear_regression__l1_ratio': 0.0},\n",
              "  {'linear_regression__alpha': 10.0, 'linear_regression__l1_ratio': 0.25},\n",
              "  {'linear_regression__alpha': 10.0, 'linear_regression__l1_ratio': 0.5},\n",
              "  {'linear_regression__alpha': 10.0, 'linear_regression__l1_ratio': 0.75},\n",
              "  {'linear_regression__alpha': 10.0, 'linear_regression__l1_ratio': 1.0}],\n",
              " 'split0_test_score': array([0.89820373, 0.89817823, 0.89816039, 0.8981498 , 0.8972019 ,\n",
              "        0.89604005, 0.89684077, 0.89760545, 0.89817204, 0.89720561,\n",
              "        0.87985004, 0.8829966 , 0.88676922, 0.89179961, 0.89725821,\n",
              "        0.81931   , 0.83334178, 0.84843362, 0.86587031, 0.89774385,\n",
              "        0.49251124, 0.5583229 , 0.64071067, 0.74431681, 0.90077569]),\n",
              " 'split1_test_score': array([0.91876522, 0.91836472, 0.91767797, 0.91614825, 0.9103958 ,\n",
              "        0.92223783, 0.92175385, 0.92102479, 0.91991459, 0.91040134,\n",
              "        0.91786324, 0.91975653, 0.92159824, 0.92289739, 0.91045103,\n",
              "        0.87347017, 0.88376093, 0.89484602, 0.90785718, 0.91093785,\n",
              "        0.57834351, 0.64456703, 0.7237721 , 0.81514486, 0.91506699]),\n",
              " 'split2_test_score': array([0.79738995, 0.7965267 , 0.79528523, 0.79327044, 0.79032004,\n",
              "        0.80004072, 0.80050326, 0.80063805, 0.79975383, 0.79085941,\n",
              "        0.78911675, 0.7905173 , 0.79272981, 0.79657447, 0.79595065,\n",
              "        0.77553836, 0.779625  , 0.78295991, 0.78576644, 0.79691806,\n",
              "        0.54116063, 0.60018138, 0.66839299, 0.74034524, 0.80141962]),\n",
              " 'split3_test_score': array([0.78739925, 0.78680229, 0.7856312 , 0.78288754, 0.77402031,\n",
              "        0.78404969, 0.78542117, 0.78681664, 0.78793406, 0.77406031,\n",
              "        0.76370316, 0.76673041, 0.77092921, 0.77753759, 0.77407171,\n",
              "        0.73451873, 0.74021515, 0.74618167, 0.75407185, 0.77426245,\n",
              "        0.50539133, 0.5610359 , 0.62598479, 0.69633658, 0.77664916]),\n",
              " 'split4_test_score': array([0.91420348, 0.91419347, 0.91402096, 0.91274794, 0.90555653,\n",
              "        0.91606435, 0.91577774, 0.91525452, 0.91447885, 0.90550225,\n",
              "        0.90761507, 0.91015083, 0.91290768, 0.91559709, 0.90535981,\n",
              "        0.85782245, 0.86889932, 0.88108029, 0.89569579, 0.90589888,\n",
              "        0.56397157, 0.6292471 , 0.70733511, 0.79797247, 0.90924976]),\n",
              " 'mean_test_score': array([0.86319233, 0.86281308, 0.86215515, 0.86064079, 0.85549892,\n",
              "        0.86368653, 0.86405936, 0.86426789, 0.86405067, 0.85560578,\n",
              "        0.85162965, 0.85403034, 0.85698683, 0.86088123, 0.85661828,\n",
              "        0.81213194, 0.82116844, 0.8307003 , 0.84185231, 0.85715222,\n",
              "        0.53627566, 0.59867086, 0.67323913, 0.75882319, 0.86063224]),\n",
              " 'std_test_score': array([0.05829378, 0.05856304, 0.05898597, 0.05964477, 0.06024221,\n",
              "        0.05934856, 0.05882416, 0.05827492, 0.05788932, 0.06010743,\n",
              "        0.06317686, 0.06318808, 0.06280708, 0.06144477, 0.05902508,\n",
              "        0.05150617, 0.05408057, 0.05725352, 0.06112882, 0.0590181 ,\n",
              "        0.0329539 , 0.03489411, 0.03749847, 0.04280477, 0.05915673]),\n",
              " 'rank_test_score': array([ 5,  6,  7,  9, 15,  4,  2,  1,  3, 14, 17, 16, 12,  8, 13, 21, 20,\n",
              "        19, 18, 11, 25, 24, 23, 22, 10], dtype=int32)}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gscv_fitted.cv_results_['mean_test_score']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PimZR78opSO",
        "outputId": "68afd774-5752-4660-de17-fbf7fc39dba5"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.86319233, 0.86281308, 0.86215515, 0.86064079, 0.85549892,\n",
              "       0.86368653, 0.86405936, 0.86426789, 0.86405067, 0.85560578,\n",
              "       0.85162965, 0.85403034, 0.85698683, 0.86088123, 0.85661828,\n",
              "       0.81213194, 0.82116844, 0.8307003 , 0.84185231, 0.85715222,\n",
              "       0.53627566, 0.59867086, 0.67323913, 0.75882319, 0.86063224])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(data = {\"degrees\": np.logspace(-3, 1, num = 5), \"scores\": gscv_fitted.cv_results_['mean_test_score']}).sort_values(by = \"scores\", ascending = False).head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "FLEoEh6jpdgZ",
        "outputId": "3c7bdbce-05a9-451e-f7be-f7aab98571a0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "All arrays must be of the same length",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-44a217568a7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"degrees\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"scores\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgscv_fitted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"scores\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using lasso instead of ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "lr_pipeline_2 = Pipeline(\n",
        "  [(\"preprocessing\", ct),\n",
        "  (\"linear_regression\", Lasso())]\n",
        ")\n",
        "\n",
        "degrees = {'linear_regression__alpha': np.logspace(-3, 1, num=5) }\n",
        "\n",
        "gscv = GridSearchCV(lr_pipeline_2, degrees, cv = 5, scoring='r2')\n"
      ],
      "metadata": {
        "id": "uZ9RmwE6prux"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gscv_fitted = gscv.fit(X, y)\n",
        "\n",
        "gscv_fitted.cv_results_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YVHA4UVpxxg",
        "outputId": "8571aa69-3474-44d3-cee5-214335162783"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([1.23605466, 0.89538627, 1.33932385, 1.26627584, 0.30861526]),\n",
              " 'std_fit_time': array([0.73482926, 0.00948104, 0.67018306, 0.50530739, 0.1165053 ]),\n",
              " 'mean_score_time': array([0.02865081, 0.0314332 , 0.03957882, 0.03501143, 0.02886138]),\n",
              " 'std_score_time': array([0.0022772 , 0.00246166, 0.01070411, 0.01262916, 0.00170874]),\n",
              " 'param_linear_regression__alpha': masked_array(data=[0.001, 0.01, 0.1, 1.0, 10.0],\n",
              "              mask=[False, False, False, False, False],\n",
              "        fill_value=1e+20),\n",
              " 'params': [{'linear_regression__alpha': 0.001},\n",
              "  {'linear_regression__alpha': 0.01},\n",
              "  {'linear_regression__alpha': 0.1},\n",
              "  {'linear_regression__alpha': 1.0},\n",
              "  {'linear_regression__alpha': 10.0}],\n",
              " 'split0_test_score': array([0.8972019 , 0.89720561, 0.89725821, 0.89774385, 0.90077569]),\n",
              " 'split1_test_score': array([0.9103958 , 0.91040134, 0.91045103, 0.91093785, 0.91506699]),\n",
              " 'split2_test_score': array([0.79032004, 0.79085941, 0.79595065, 0.79691806, 0.80141962]),\n",
              " 'split3_test_score': array([0.77402031, 0.77406031, 0.77407171, 0.77426245, 0.77664916]),\n",
              " 'split4_test_score': array([0.90555653, 0.90550225, 0.90535981, 0.90589888, 0.90924976]),\n",
              " 'mean_test_score': array([0.85549892, 0.85560578, 0.85661828, 0.85715222, 0.86063224]),\n",
              " 'std_test_score': array([0.06024221, 0.06010743, 0.05902508, 0.0590181 , 0.05915673]),\n",
              " 'rank_test_score': array([5, 4, 3, 2, 1], dtype=int32)}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gscv_fitted.cv_results_['mean_test_score']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Aa0em9Bp9-F",
        "outputId": "196fee0c-e8ea-4d10-a55a-cc35c4253d7b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.85549892, 0.85560578, 0.85661828, 0.85715222, 0.86063224])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#elastic net tuning\n",
        "\n",
        "lr_pipeline_2 = Pipeline(\n",
        "  [(\"preprocessing\", ct),\n",
        "  (\"linear_regression\", ElasticNet())]\n",
        ")\n",
        "\n",
        "degrees = {\n",
        "    'linear_regression__alpha': np.logspace(-3, 1, num=5),\n",
        "    'linear_regression__l1_ratio': np.linspace(0, 1, num=5)\n",
        "     }\n",
        "\n",
        "gscv = GridSearchCV(lr_pipeline_2, degrees, cv = 5, scoring='r2')\n"
      ],
      "metadata": {
        "id": "wQ40mRLKp_l2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gscv_fitted = gscv.fit(X, y)\n",
        "\n",
        "gscv_fitted.cv_results_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r74Ms8oUr-4I",
        "outputId": "002b5c0f-8816-42f2-f6ef-8412332f3995"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([1.40802474, 0.82172384, 0.83770356, 1.39689145, 0.86147265,\n",
              "        1.37794037, 0.84352455, 0.77637458, 1.28926158, 0.87690043,\n",
              "        1.31312151, 0.26258893, 0.19854741, 0.24252186, 0.84102535,\n",
              "        1.52111754, 0.1324563 , 0.14164205, 0.1539371 , 0.71318569,\n",
              "        1.5223681 , 0.10950193, 0.11021819, 0.11824675, 0.29897509]),\n",
              " 'std_fit_time': array([0.38262799, 0.05436514, 0.0334284 , 0.79775532, 0.02065184,\n",
              "        0.8345447 , 0.21833267, 0.04108655, 0.74930414, 0.02936257,\n",
              "        0.95696092, 0.13685251, 0.01544117, 0.0131766 , 0.09882871,\n",
              "        0.82388147, 0.0133888 , 0.01035912, 0.0079781 , 0.12507164,\n",
              "        0.75062709, 0.01942079, 0.01526828, 0.00634105, 0.10563343]),\n",
              " 'mean_score_time': array([0.03375039, 0.03006663, 0.03038697, 0.03807764, 0.02984176,\n",
              "        0.04473648, 0.03183417, 0.03202434, 0.02930822, 0.02990642,\n",
              "        0.0406939 , 0.03416071, 0.03026185, 0.03374901, 0.03136792,\n",
              "        0.03713675, 0.02963743, 0.02916074, 0.02868972, 0.02849598,\n",
              "        0.03559904, 0.03689561, 0.03651929, 0.02787547, 0.03085351]),\n",
              " 'std_score_time': array([0.00437141, 0.00364411, 0.00401982, 0.01097013, 0.00154625,\n",
              "        0.01810034, 0.00095395, 0.00796145, 0.00147511, 0.0031188 ,\n",
              "        0.01222051, 0.01071126, 0.00303501, 0.00533701, 0.00272996,\n",
              "        0.01238143, 0.00168781, 0.00197461, 0.00228299, 0.00156304,\n",
              "        0.01206516, 0.01173142, 0.0096328 , 0.00079554, 0.00272088]),\n",
              " 'param_linear_regression__alpha': masked_array(data=[0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01,\n",
              "                    0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 1.0, 1.0, 1.0,\n",
              "                    1.0, 1.0, 10.0, 10.0, 10.0, 10.0, 10.0],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False],\n",
              "        fill_value=1e+20),\n",
              " 'param_linear_regression__l1_ratio': masked_array(data=[0.0, 0.25, 0.5, 0.75, 1.0, 0.0, 0.25, 0.5, 0.75, 1.0,\n",
              "                    0.0, 0.25, 0.5, 0.75, 1.0, 0.0, 0.25, 0.5, 0.75, 1.0,\n",
              "                    0.0, 0.25, 0.5, 0.75, 1.0],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False],\n",
              "        fill_value=1e+20),\n",
              " 'params': [{'linear_regression__alpha': 0.001,\n",
              "   'linear_regression__l1_ratio': 0.0},\n",
              "  {'linear_regression__alpha': 0.001, 'linear_regression__l1_ratio': 0.25},\n",
              "  {'linear_regression__alpha': 0.001, 'linear_regression__l1_ratio': 0.5},\n",
              "  {'linear_regression__alpha': 0.001, 'linear_regression__l1_ratio': 0.75},\n",
              "  {'linear_regression__alpha': 0.001, 'linear_regression__l1_ratio': 1.0},\n",
              "  {'linear_regression__alpha': 0.01, 'linear_regression__l1_ratio': 0.0},\n",
              "  {'linear_regression__alpha': 0.01, 'linear_regression__l1_ratio': 0.25},\n",
              "  {'linear_regression__alpha': 0.01, 'linear_regression__l1_ratio': 0.5},\n",
              "  {'linear_regression__alpha': 0.01, 'linear_regression__l1_ratio': 0.75},\n",
              "  {'linear_regression__alpha': 0.01, 'linear_regression__l1_ratio': 1.0},\n",
              "  {'linear_regression__alpha': 0.1, 'linear_regression__l1_ratio': 0.0},\n",
              "  {'linear_regression__alpha': 0.1, 'linear_regression__l1_ratio': 0.25},\n",
              "  {'linear_regression__alpha': 0.1, 'linear_regression__l1_ratio': 0.5},\n",
              "  {'linear_regression__alpha': 0.1, 'linear_regression__l1_ratio': 0.75},\n",
              "  {'linear_regression__alpha': 0.1, 'linear_regression__l1_ratio': 1.0},\n",
              "  {'linear_regression__alpha': 1.0, 'linear_regression__l1_ratio': 0.0},\n",
              "  {'linear_regression__alpha': 1.0, 'linear_regression__l1_ratio': 0.25},\n",
              "  {'linear_regression__alpha': 1.0, 'linear_regression__l1_ratio': 0.5},\n",
              "  {'linear_regression__alpha': 1.0, 'linear_regression__l1_ratio': 0.75},\n",
              "  {'linear_regression__alpha': 1.0, 'linear_regression__l1_ratio': 1.0},\n",
              "  {'linear_regression__alpha': 10.0, 'linear_regression__l1_ratio': 0.0},\n",
              "  {'linear_regression__alpha': 10.0, 'linear_regression__l1_ratio': 0.25},\n",
              "  {'linear_regression__alpha': 10.0, 'linear_regression__l1_ratio': 0.5},\n",
              "  {'linear_regression__alpha': 10.0, 'linear_regression__l1_ratio': 0.75},\n",
              "  {'linear_regression__alpha': 10.0, 'linear_regression__l1_ratio': 1.0}],\n",
              " 'split0_test_score': array([0.89820373, 0.89817823, 0.89816039, 0.8981498 , 0.8972019 ,\n",
              "        0.89604005, 0.89684077, 0.89760545, 0.89817204, 0.89720561,\n",
              "        0.87985004, 0.8829966 , 0.88676922, 0.89179961, 0.89725821,\n",
              "        0.81931   , 0.83334178, 0.84843362, 0.86587031, 0.89774385,\n",
              "        0.49251124, 0.5583229 , 0.64071067, 0.74431681, 0.90077569]),\n",
              " 'split1_test_score': array([0.91876522, 0.91836472, 0.91767797, 0.91614825, 0.9103958 ,\n",
              "        0.92223783, 0.92175385, 0.92102479, 0.91991459, 0.91040134,\n",
              "        0.91786324, 0.91975653, 0.92159824, 0.92289739, 0.91045103,\n",
              "        0.87347017, 0.88376093, 0.89484602, 0.90785718, 0.91093785,\n",
              "        0.57834351, 0.64456703, 0.7237721 , 0.81514486, 0.91506699]),\n",
              " 'split2_test_score': array([0.79738995, 0.7965267 , 0.79528523, 0.79327044, 0.79032004,\n",
              "        0.80004072, 0.80050326, 0.80063805, 0.79975383, 0.79085941,\n",
              "        0.78911675, 0.7905173 , 0.79272981, 0.79657447, 0.79595065,\n",
              "        0.77553836, 0.779625  , 0.78295991, 0.78576644, 0.79691806,\n",
              "        0.54116063, 0.60018138, 0.66839299, 0.74034524, 0.80141962]),\n",
              " 'split3_test_score': array([0.78739925, 0.78680229, 0.7856312 , 0.78288754, 0.77402031,\n",
              "        0.78404969, 0.78542117, 0.78681664, 0.78793406, 0.77406031,\n",
              "        0.76370316, 0.76673041, 0.77092921, 0.77753759, 0.77407171,\n",
              "        0.73451873, 0.74021515, 0.74618167, 0.75407185, 0.77426245,\n",
              "        0.50539133, 0.5610359 , 0.62598479, 0.69633658, 0.77664916]),\n",
              " 'split4_test_score': array([0.91420348, 0.91419347, 0.91402096, 0.91274794, 0.90555653,\n",
              "        0.91606435, 0.91577774, 0.91525452, 0.91447885, 0.90550225,\n",
              "        0.90761507, 0.91015083, 0.91290768, 0.91559709, 0.90535981,\n",
              "        0.85782245, 0.86889932, 0.88108029, 0.89569579, 0.90589888,\n",
              "        0.56397157, 0.6292471 , 0.70733511, 0.79797247, 0.90924976]),\n",
              " 'mean_test_score': array([0.86319233, 0.86281308, 0.86215515, 0.86064079, 0.85549892,\n",
              "        0.86368653, 0.86405936, 0.86426789, 0.86405067, 0.85560578,\n",
              "        0.85162965, 0.85403034, 0.85698683, 0.86088123, 0.85661828,\n",
              "        0.81213194, 0.82116844, 0.8307003 , 0.84185231, 0.85715222,\n",
              "        0.53627566, 0.59867086, 0.67323913, 0.75882319, 0.86063224]),\n",
              " 'std_test_score': array([0.05829378, 0.05856304, 0.05898597, 0.05964477, 0.06024221,\n",
              "        0.05934856, 0.05882416, 0.05827492, 0.05788932, 0.06010743,\n",
              "        0.06317686, 0.06318808, 0.06280708, 0.06144477, 0.05902508,\n",
              "        0.05150617, 0.05408057, 0.05725352, 0.06112882, 0.0590181 ,\n",
              "        0.0329539 , 0.03489411, 0.03749847, 0.04280477, 0.05915673]),\n",
              " 'rank_test_score': array([ 5,  6,  7,  9, 15,  4,  2,  1,  3, 14, 17, 16, 12,  8, 13, 21, 20,\n",
              "        19, 18, 11, 25, 24, 23, 22, 10], dtype=int32)}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score = gscv_fitted.cv_results_['mean_test_score']\n",
        "specifications = gscv_fitted.cv_results_['params']\n",
        "best_model = pd.DataFrame(data = {\"r2_score\": r2_score, \"specifications\": specifications})\n",
        "best_model.sort_values(by = \"r2_score\", ascending = False).head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "yk5efpTFssFI",
        "outputId": "db544048-0f3f-4f92-9050-80c0752625dd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   r2_score                                     specifications\n",
              "7  0.864268  {'linear_regression__alpha': 0.01, 'linear_reg..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-31e7d0a8-ca84-48ce-b3e5-3f06d0a28e9e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>r2_score</th>\n",
              "      <th>specifications</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.864268</td>\n",
              "      <td>{'linear_regression__alpha': 0.01, 'linear_reg...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31e7d0a8-ca84-48ce-b3e5-3f06d0a28e9e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-31e7d0a8-ca84-48ce-b3e5-3f06d0a28e9e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-31e7d0a8-ca84-48ce-b3e5-3f06d0a28e9e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"best_model\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"r2_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.8642678883652006,\n        \"max\": 0.8642678883652006,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.8642678883652006\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"specifications\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}